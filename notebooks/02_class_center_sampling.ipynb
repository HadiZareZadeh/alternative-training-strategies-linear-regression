{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment A: Class-Center-Based Sampling\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook explores an alternative training strategy: **class-center-based sampling**. The core idea is to discretize the continuous target variable into pseudo-classes, then use class-center samples (mean, median, closest-to-center, synthetic) to augment or replace traditional mini-batch training.\n",
        "\n",
        "## Research Question\n",
        "\n",
        "Can class-center-based samples replace or augment mini-batch training to improve convergence speed and final error?\n",
        "\n",
        "## Methodology\n",
        "\n",
        "1. **Discretization**: Divide continuous target into K bins (pseudo-classes)\n",
        "2. **Class-Center Creation**: For each class, create:\n",
        "   - Mean vector (average of all features in that class)\n",
        "   - Median vector (median of all features in that class)\n",
        "   - Closest-to-center real sample\n",
        "   - Synthetic center sample\n",
        "3. **Training Strategies**: Compare\n",
        "   - Only class-center samples\n",
        "   - Class-center + random samples\n",
        "   - Class-center + mini-batch samples\n",
        "\n",
        "## Hypothesis\n",
        "\n",
        "Class-center samples may provide a more stable gradient signal by representing the \"typical\" sample in each target range, potentially leading to faster convergence or better generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data and Baseline Model\n",
        "\n",
        "We'll reuse the data and baseline model from the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set: {X_train_scaled.shape[0]} samples\")\n",
        "print(f\"Target range: [{y_train.min():.2f}, {y_train.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class-Center Sampling Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClassCenterSampler:\n",
        "    \"\"\"\n",
        "    Creates class-center samples by discretizing continuous targets.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_bins=10):\n",
        "        self.n_bins = n_bins\n",
        "        self.bin_edges = None\n",
        "        self.class_centers = {}\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Discretize targets and compute class centers.\"\"\"\n",
        "        # Convert to numpy arrays for consistent indexing\n",
        "        if hasattr(X, 'values'):\n",
        "            X = X.values\n",
        "        if hasattr(y, 'values'):\n",
        "            y = y.values\n",
        "        \n",
        "        # Create bins\n",
        "        _, self.bin_edges = pd.cut(y, bins=self.n_bins, retbins=True, duplicates='drop')\n",
        "        \n",
        "        # Assign samples to classes\n",
        "        y_binned = pd.cut(y, bins=self.bin_edges, labels=False, include_lowest=True)\n",
        "        \n",
        "        # Compute class centers for each bin\n",
        "        for class_id in range(self.n_bins):\n",
        "            mask = y_binned == class_id\n",
        "            if mask.sum() > 0:\n",
        "                X_class = X[mask]\n",
        "                y_class = y[mask]\n",
        "                \n",
        "                # Mean vector\n",
        "                mean_vector = X_class.mean(axis=0)\n",
        "                mean_target = y_class.mean()\n",
        "                \n",
        "                # Median vector\n",
        "                median_vector = np.median(X_class, axis=0)\n",
        "                median_target = np.median(y_class)\n",
        "                \n",
        "                # Closest-to-center real sample\n",
        "                center_point = X_class.mean(axis=0)\n",
        "                distances = np.linalg.norm(X_class - center_point, axis=1)\n",
        "                closest_idx = np.argmin(distances)\n",
        "                closest_vector = X_class[closest_idx]\n",
        "                closest_target = y_class[closest_idx]\n",
        "                \n",
        "                # Synthetic center (interpolation between mean and median)\n",
        "                synthetic_vector = (mean_vector + median_vector) / 2\n",
        "                synthetic_target = (mean_target + median_target) / 2\n",
        "                \n",
        "                self.class_centers[class_id] = {\n",
        "                    'mean': (mean_vector, mean_target),\n",
        "                    'median': (median_vector, median_target),\n",
        "                    'closest': (closest_vector, closest_target),\n",
        "                    'synthetic': (synthetic_vector, synthetic_target),\n",
        "                    'samples': (X_class, y_class)\n",
        "                }\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def get_class_centers(self, method='mean'):\n",
        "        \"\"\"Get class center samples.\"\"\"\n",
        "        X_centers = []\n",
        "        y_centers = []\n",
        "        \n",
        "        for class_id in sorted(self.class_centers.keys()):\n",
        "            if method in self.class_centers[class_id]:\n",
        "                X_vec, y_val = self.class_centers[class_id][method]\n",
        "                X_centers.append(X_vec)\n",
        "                y_centers.append(y_val)\n",
        "        \n",
        "        return np.array(X_centers), np.array(y_centers)\n",
        "    \n",
        "    def get_all_centers(self):\n",
        "        \"\"\"Get all types of class centers.\"\"\"\n",
        "        all_X = []\n",
        "        all_y = []\n",
        "        \n",
        "        for method in ['mean', 'median', 'closest', 'synthetic']:\n",
        "            X_m, y_m = self.get_class_centers(method)\n",
        "            all_X.append(X_m)\n",
        "            all_y.append(y_m)\n",
        "        \n",
        "        return np.vstack(all_X), np.hstack(all_y)\n",
        "\n",
        "print(\"ClassCenterSampler defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Class Centers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create class centers\n",
        "sampler = ClassCenterSampler(n_bins=10)\n",
        "sampler.fit(pd.DataFrame(X_train_scaled), pd.Series(y_train))\n",
        "\n",
        "# Visualize class distribution\n",
        "y_binned = pd.cut(y_train, bins=sampler.bin_edges, labels=False, include_lowest=True)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(y_binned, bins=sampler.n_bins, edgecolor='black', alpha=0.7)\n",
        "plt.xlabel('Pseudo-Class ID', fontsize=12)\n",
        "plt.ylabel('Number of Samples', fontsize=12)\n",
        "plt.title('Distribution of Samples Across Pseudo-Classes', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Created {len(sampler.class_centers)} class centers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modified Linear Regression with Class-Center Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearRegressionClassCenter:\n",
        "    \"\"\"Linear Regression with class-center sampling support.\"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000, batch_size=32, random_state=42):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.batch_size = batch_size\n",
        "        self.random_state = random_state\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []\n",
        "        \n",
        "    def _initialize_parameters(self, n_features):\n",
        "        np.random.seed(self.random_state)\n",
        "        self.weights = np.random.randn(n_features) * 0.01\n",
        "        self.bias = 0.0\n",
        "        \n",
        "    def _compute_loss(self, X, y):\n",
        "        predictions = X @ self.weights + self.bias\n",
        "        return np.mean((y - predictions) ** 2)\n",
        "    \n",
        "    def _compute_gradients(self, X_batch, y_batch):\n",
        "        n_samples = X_batch.shape[0]\n",
        "        predictions = X_batch @ self.weights + self.bias\n",
        "        error = predictions - y_batch\n",
        "        dw = (2 / n_samples) * (X_batch.T @ error)\n",
        "        db = (2 / n_samples) * np.sum(error)\n",
        "        return dw, db\n",
        "    \n",
        "    def fit_class_centers_only(self, X_centers, y_centers, verbose=True):\n",
        "        \"\"\"Train using only class-center samples.\"\"\"\n",
        "        n_samples, n_features = X_centers.shape\n",
        "        self._initialize_parameters(n_features)\n",
        "        \n",
        "        for iteration in range(self.n_iterations):\n",
        "            # Use all centers (small dataset, so use all)\n",
        "            dw, db = self._compute_gradients(X_centers, y_centers)\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "            \n",
        "            loss = self._compute_loss(X_centers, y_centers)\n",
        "            self.loss_history.append(loss)\n",
        "            \n",
        "            if verbose and (iteration + 1) % 100 == 0:\n",
        "                print(f\"Iteration {iteration + 1}/{self.n_iterations}, Loss: {loss:.4f}\")\n",
        "    \n",
        "    def fit_mixed(self, X_centers, y_centers, X_train, y_train, mix_ratio=0.5, verbose=True):\n",
        "        \"\"\"Train using mix of class centers and training data.\"\"\"\n",
        "        n_samples, n_features = X_train.shape\n",
        "        self._initialize_parameters(n_features)\n",
        "        \n",
        "        for iteration in range(self.n_iterations):\n",
        "            # Create mixed batch\n",
        "            n_center_samples = int(self.batch_size * mix_ratio)\n",
        "            n_train_samples = self.batch_size - n_center_samples\n",
        "            \n",
        "            # Sample from centers\n",
        "            center_indices = np.random.choice(len(X_centers), n_center_samples, replace=True)\n",
        "            X_batch_center = X_centers[center_indices]\n",
        "            y_batch_center = y_centers[center_indices]\n",
        "            \n",
        "            # Sample from training data\n",
        "            train_indices = np.random.choice(n_samples, n_train_samples, replace=False)\n",
        "            X_batch_train = X_train[train_indices]\n",
        "            y_batch_train = y_train[train_indices]\n",
        "            \n",
        "            # Combine\n",
        "            X_batch = np.vstack([X_batch_center, X_batch_train])\n",
        "            y_batch = np.hstack([y_batch_center, y_batch_train])\n",
        "            \n",
        "            # Update\n",
        "            dw, db = self._compute_gradients(X_batch, y_batch)\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "            \n",
        "            # Compute loss on full training set\n",
        "            loss = self._compute_loss(X_train, y_train)\n",
        "            self.loss_history.append(loss)\n",
        "            \n",
        "            if verbose and (iteration + 1) % 100 == 0:\n",
        "                print(f\"Iteration {iteration + 1}/{self.n_iterations}, Loss: {loss:.4f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return X @ self.weights + self.bias\n",
        "\n",
        "print(\"LinearRegressionClassCenter defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 1: Training with Class Centers Only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all class centers\n",
        "X_centers_all, y_centers_all = sampler.get_all_centers()\n",
        "print(f\"Total class center samples: {len(X_centers_all)}\")\n",
        "\n",
        "# Train with all centers\n",
        "model_centers_only = LinearRegressionClassCenter(\n",
        "    learning_rate=0.01,\n",
        "    n_iterations=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nTraining with class centers only...\")\n",
        "model_centers_only.fit_class_centers_only(X_centers_all, y_centers_all, verbose=True)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model_centers_only.predict(X_test_scaled)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n=== Class Centers Only ===\")\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "print(f\"Test R²: {test_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 2: Mixed Training (Class Centers + Random Samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with mix of centers and random samples\n",
        "model_mixed = LinearRegressionClassCenter(\n",
        "    learning_rate=0.01,\n",
        "    n_iterations=1000,\n",
        "    batch_size=32,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training with class centers + random samples (50/50 mix)...\")\n",
        "model_mixed.fit_mixed(X_centers_all, y_centers_all, X_train_scaled, y_train, mix_ratio=0.5, verbose=True)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model_mixed.predict(X_test_scaled)\n",
        "test_rmse_mixed = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "test_r2_mixed = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n=== Mixed Training (Centers + Random) ===\")\n",
        "print(f\"Test RMSE: {test_rmse_mixed:.4f}\")\n",
        "print(f\"Test R²: {test_r2_mixed:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results using pickle\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "results_dir = os.path.join(os.path.dirname(os.getcwd()), 'results')\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "class_center_results = {\n",
        "    'centers_only': {\n",
        "        'method': 'Class Centers Only',\n",
        "        'test_rmse': test_rmse,\n",
        "        'test_r2': test_r2,\n",
        "        'loss_history': model_centers_only.loss_history.copy()\n",
        "    },\n",
        "    'mixed': {\n",
        "        'method': 'Mixed (Centers + Random)',\n",
        "        'test_rmse': test_rmse_mixed,\n",
        "        'test_r2': test_r2_mixed,\n",
        "        'loss_history': model_mixed.loss_history.copy()\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(results_dir, '02_class_center_results.pkl'), 'wb') as f:\n",
        "    pickle.dump(class_center_results, f)\n",
        "\n",
        "print(f\"\\nResults saved to: {os.path.join(results_dir, '02_class_center_results.pkl')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization: Comparison of Training Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load baseline results (from previous notebook)\n",
        "# For now, we'll create a simple baseline here\n",
        "from sklearn.linear_model import LinearRegression as SklearnLR\n",
        "baseline_sklearn = SklearnLR()\n",
        "baseline_sklearn.fit(X_train_scaled, y_train)\n",
        "y_pred_baseline = baseline_sklearn.predict(X_test_scaled)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(model_centers_only.loss_history, label='Class Centers Only', linewidth=2)\n",
        "plt.plot(model_mixed.loss_history, label='Mixed (Centers + Random)', linewidth=2)\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('MSE Loss', fontsize=12)\n",
        "plt.title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "methods = ['Baseline\\n(Sklearn)', 'Centers Only', 'Mixed Training']\n",
        "rmses = [baseline_rmse, test_rmse, test_rmse_mixed]\n",
        "plt.bar(methods, rmses, color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.7, edgecolor='black')\n",
        "plt.ylabel('Test RMSE', fontsize=12)\n",
        "plt.title('Test Performance Comparison', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=15)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(f\"Baseline RMSE: {baseline_rmse:.4f}\")\n",
        "print(f\"Class Centers Only RMSE: {test_rmse:.4f}\")\n",
        "print(f\"Mixed Training RMSE: {test_rmse_mixed:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis and Conclusions\n",
        "\n",
        "### Key Observations:\n",
        "\n",
        "1. **Class Centers Only**: Training with only class-center samples provides a condensed representation but may lack diversity.\n",
        "2. **Mixed Training**: Combining class centers with random samples balances stability and diversity.\n",
        "3. **Convergence Behavior**: Class-center sampling may converge faster initially but could plateau earlier.\n",
        "\n",
        "### Research Insights:\n",
        "\n",
        "- Class-center sampling offers an interesting data-centric perspective on training\n",
        "- The effectiveness depends on the dataset structure and class distribution\n",
        "- Further experiments with different mix ratios and center types could reveal optimal strategies"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
