{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Implementation: Standard Linear Regression with Mini-Batch Gradient Descent\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook establishes our baseline implementation of linear regression using mini-batch gradient descent with a fixed learning rate. This serves as the reference point against which we'll compare our alternative training strategies.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. Implement linear regression from scratch\n",
        "2. Implement mini-batch gradient descent\n",
        "3. Establish baseline performance metrics\n",
        "4. Visualize training dynamics (loss vs iterations)\n",
        "5. Understand convergence behavior\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "For linear regression, we aim to minimize the Mean Squared Error (MSE):\n",
        "\n",
        "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\mathbf{w}^T\\mathbf{x}_i - b)^2$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{w}$ is the weight vector\n",
        "- $b$ is the bias term\n",
        "- $\\mathbf{x}_i$ is the feature vector for sample $i$\n",
        "- $y_i$ is the true target value\n",
        "\n",
        "The gradient with respect to weights:\n",
        "$$\\frac{\\partial \\text{MSE}}{\\partial \\mathbf{w}} = -\\frac{2}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)\\mathbf{x}_i$$\n",
        "\n",
        "The gradient with respect to bias:\n",
        "$$\\frac{\\partial \\text{MSE}}{\\partial b} = -\\frac{2}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)$$\n",
        "\n",
        "In mini-batch gradient descent, we update parameters using a subset of the data:\n",
        "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial \\text{MSE}}{\\partial \\mathbf{w}}$$\n",
        "$$b \\leftarrow b - \\alpha \\frac{\\partial \\text{MSE}}{\\partial b}$$\n",
        "\n",
        "Where $\\alpha$ is the learning rate (fixed in our baseline)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better-looking plots\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "We'll use the California Housing dataset, which contains housing information from the 1990 California census."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nFeature names: {housing.feature_names}\")\n",
        "print(f\"\\nTarget statistics:\")\n",
        "print(f\"  Mean: {y.mean():.2f}\")\n",
        "print(f\"  Std: {y.std():.2f}\")\n",
        "print(f\"  Min: {y.min():.2f}\")\n",
        "print(f\"  Max: {y.max():.2f}\")\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features (important for gradient descent)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nTrain set size: {X_train_scaled.shape[0]}\")\n",
        "print(f\"Test set size: {X_test_scaled.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression Implementation\n",
        "\n",
        "We'll implement linear regression from scratch to have full control over the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "    \"\"\"\n",
        "    Linear Regression implementation with mini-batch gradient descent.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000, batch_size=32, random_state=42):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        learning_rate : float\n",
        "            Fixed learning rate for gradient descent\n",
        "        n_iterations : int\n",
        "            Number of training iterations\n",
        "        batch_size : int\n",
        "            Size of mini-batches\n",
        "        random_state : int\n",
        "            Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.batch_size = batch_size\n",
        "        self.random_state = random_state\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []\n",
        "        \n",
        "    def _initialize_parameters(self, n_features):\n",
        "        \"\"\"Initialize weights and bias.\"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "        self.weights = np.random.randn(n_features) * 0.01\n",
        "        self.bias = 0.0\n",
        "        \n",
        "    def _compute_loss(self, X, y):\n",
        "        \"\"\"Compute MSE loss.\"\"\"\n",
        "        predictions = X @ self.weights + self.bias\n",
        "        mse = np.mean((y - predictions) ** 2)\n",
        "        return mse\n",
        "    \n",
        "    def _compute_gradients(self, X_batch, y_batch):\n",
        "        \"\"\"Compute gradients for a batch.\"\"\"\n",
        "        n_samples = X_batch.shape[0]\n",
        "        predictions = X_batch @ self.weights + self.bias\n",
        "        error = predictions - y_batch\n",
        "        \n",
        "        # Gradients\n",
        "        dw = (2 / n_samples) * (X_batch.T @ error)\n",
        "        db = (2 / n_samples) * np.sum(error)\n",
        "        \n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        max_grad_norm = 10.0\n",
        "        grad_norm = np.sqrt(np.sum(dw**2) + db**2)\n",
        "        if grad_norm > max_grad_norm:\n",
        "            scale = max_grad_norm / grad_norm\n",
        "            dw *= scale\n",
        "            db *= scale\n",
        "        \n",
        "        return dw, db\n",
        "    \n",
        "    def fit(self, X, y, verbose=True):\n",
        "        \"\"\"\n",
        "        Train the model using mini-batch gradient descent.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Training features\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Training targets\n",
        "        verbose : bool\n",
        "            Whether to print progress\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self._initialize_parameters(n_features)\n",
        "        \n",
        "        # Training loop\n",
        "        for iteration in range(self.n_iterations):\n",
        "            # Shuffle data for each epoch\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "            \n",
        "            # Mini-batch gradient descent\n",
        "            epoch_losses = []\n",
        "            for i in range(0, n_samples, self.batch_size):\n",
        "                # Get batch\n",
        "                end_idx = min(i + self.batch_size, n_samples)\n",
        "                X_batch = X_shuffled[i:end_idx]\n",
        "                y_batch = y_shuffled[i:end_idx]\n",
        "                \n",
        "                # Compute gradients\n",
        "                dw, db = self._compute_gradients(X_batch, y_batch)\n",
        "                \n",
        "                # Update parameters\n",
        "                self.weights -= self.learning_rate * dw\n",
        "                self.bias -= self.learning_rate * db\n",
        "                \n",
        "                # Check for NaN/Inf to detect divergence\n",
        "                if np.any(np.isnan(self.weights)) or np.isnan(self.bias) or \\\n",
        "                   np.any(np.isinf(self.weights)) or np.isinf(self.bias):\n",
        "                    if verbose:\n",
        "                        print(f\"Warning: Divergence detected at iteration {iteration + 1}. Stopping early.\")\n",
        "                    # Fill remaining history with NaN to indicate divergence\n",
        "                    while len(self.loss_history) < self.n_iterations:\n",
        "                        self.loss_history.append(np.nan)\n",
        "                    return\n",
        "                \n",
        "                # Compute batch loss\n",
        "                batch_loss = self._compute_loss(X_batch, y_batch)\n",
        "                epoch_losses.append(batch_loss)\n",
        "            \n",
        "            # Store average loss for this epoch\n",
        "            avg_loss = np.mean(epoch_losses)\n",
        "            self.loss_history.append(avg_loss)\n",
        "            \n",
        "            if verbose and (iteration + 1) % 100 == 0:\n",
        "                print(f\"Iteration {iteration + 1}/{self.n_iterations}, Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\"\"\"\n",
        "        return X @ self.weights + self.bias\n",
        "    \n",
        "    def get_parameters(self):\n",
        "        \"\"\"Return model parameters.\"\"\"\n",
        "        return self.weights.copy(), self.bias\n",
        "\n",
        "print(\"Linear Regression class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline Training\n",
        "\n",
        "Let's train our baseline model and observe the training dynamics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train baseline model\n",
        "baseline_model = LinearRegression(\n",
        "    learning_rate=0.01,\n",
        "    n_iterations=1000,\n",
        "    batch_size=32,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training baseline model...\")\n",
        "baseline_model.fit(X_train_scaled, y_train, verbose=True)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = baseline_model.predict(X_test_scaled)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n=== Baseline Model Performance ===\")\n",
        "print(f\"Test MSE: {test_mse:.4f}\")\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "print(f\"Test RÂ²: {test_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization: Training Dynamics\n",
        "\n",
        "Let's visualize how the loss decreases over iterations. This will serve as our baseline for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(baseline_model.loss_history, linewidth=2)\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('MSE Loss', fontsize=12)\n",
        "plt.title('Training Loss: Baseline Model', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(baseline_model.loss_history, linewidth=2)\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('MSE Loss (Log Scale)', fontsize=12)\n",
        "plt.title('Training Loss: Baseline Model (Log Scale)', fontsize=14, fontweight='bold')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print convergence statistics\n",
        "print(\"=== Convergence Analysis ===\")\n",
        "print(f\"Initial Loss: {baseline_model.loss_history[0]:.4f}\")\n",
        "print(f\"Final Loss: {baseline_model.loss_history[-1]:.4f}\")\n",
        "print(f\"Loss Reduction: {((baseline_model.loss_history[0] - baseline_model.loss_history[-1]) / baseline_model.loss_history[0] * 100):.2f}%\")\n",
        "print(f\"Convergence Rate (last 100 iterations): {np.mean(np.diff(baseline_model.loss_history[-100:])):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Rate Sensitivity Analysis\n",
        "\n",
        "Before moving to alternative strategies, let's understand how sensitive our baseline is to learning rate. This will help us contextualize the adaptive learning rate experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different learning rates\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
        "lr_results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model = LinearRegression(\n",
        "        learning_rate=lr,\n",
        "        n_iterations=1000,\n",
        "        batch_size=32,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train_scaled, y_train, verbose=False)\n",
        "    \n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    \n",
        "    # Check for NaN predictions (divergence)\n",
        "    if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)):\n",
        "        print(f\"LR={lr:.3f}: Diverged (NaN/Inf predictions)\")\n",
        "        lr_results[lr] = {\n",
        "            'loss_history': model.loss_history.copy(),\n",
        "            'test_rmse': np.nan\n",
        "        }\n",
        "    else:\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        lr_results[lr] = {\n",
        "            'loss_history': model.loss_history.copy(),\n",
        "            'test_rmse': test_rmse\n",
        "        }\n",
        "        print(f\"LR={lr:.3f}: Test RMSE={test_rmse:.4f}\")\n",
        "\n",
        "# Visualize learning rate comparison\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for lr, results in lr_results.items():\n",
        "    # Filter out NaN values for plotting\n",
        "    loss_history = np.array(results['loss_history'])\n",
        "    valid_mask = ~np.isnan(loss_history)\n",
        "    if np.any(valid_mask):\n",
        "        plt.plot(np.where(valid_mask)[0], loss_history[valid_mask], label=f'LR={lr}', linewidth=2)\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('MSE Loss', fontsize=12)\n",
        "plt.title('Learning Rate Sensitivity: Training Loss', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "lrs = list(lr_results.keys())\n",
        "rmses = [lr_results[lr]['test_rmse'] for lr in lrs]\n",
        "# Filter out NaN values for plotting\n",
        "valid_data = [(lr, rmse) for lr, rmse in zip(lrs, rmses) if not np.isnan(rmse)]\n",
        "if valid_data:\n",
        "    valid_lrs, valid_rmses = zip(*valid_data)\n",
        "    plt.plot(valid_lrs, valid_rmses, marker='o', linewidth=2, markersize=10)\n",
        "plt.xlabel('Learning Rate', fontsize=12)\n",
        "plt.ylabel('Test RMSE', fontsize=12)\n",
        "plt.title('Learning Rate vs Test Performance', fontsize=14, fontweight='bold')\n",
        "plt.xscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Store baseline results for comparison\n",
        "baseline_results = {\n",
        "    'model': baseline_model,\n",
        "    'test_rmse': test_rmse,\n",
        "    'test_r2': test_r2,\n",
        "    'loss_history': baseline_model.loss_history.copy(),\n",
        "    'learning_rate': 0.01\n",
        "}\n",
        "\n",
        "print(\"\\n=== Baseline Established ===\")\n",
        "print(\"We'll use this as our reference point for comparing alternative training strategies.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results using pickle\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "results_dir = os.path.join(os.path.dirname(os.getcwd()), 'results')\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Update baseline_results to include all necessary info\n",
        "baseline_results['method'] = 'Baseline (Fixed LR)'\n",
        "baseline_results['test_mse'] = test_mse\n",
        "baseline_results['final_loss'] = baseline_model.loss_history[-1]\n",
        "baseline_results['initial_loss'] = baseline_model.loss_history[0]\n",
        "# Remove model object (not serializable)\n",
        "baseline_results.pop('model', None)\n",
        "\n",
        "with open(os.path.join(results_dir, '01_baseline_results.pkl'), 'wb') as f:\n",
        "    pickle.dump(baseline_results, f)\n",
        "\n",
        "print(f\"\\nResults saved to: {os.path.join(results_dir, '01_baseline_results.pkl')}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
