{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment B: Adaptive Learning Rate as a Dynamic Parameter\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook explores treating learning rate as a **dynamic parameter** that adapts during training, rather than a fixed hyperparameter. We implement an adaptive learning rate strategy that monitors loss spikes and automatically adjusts the learning rate.\n",
        "\n",
        "## Research Question\n",
        "\n",
        "Can learning rate be treated as a dynamic parameter instead of a fixed hyperparameter to improve convergence speed and stability?\n",
        "\n",
        "## Methodology\n",
        "\n",
        "1. **Start with high learning rate**: Begin training with a relatively high learning rate\n",
        "2. **Monitor loss spikes**: Track MSE/RMSE for sudden increases\n",
        "3. **Adaptive reduction**: When a spike is detected, reduce learning rate by:\n",
        "   - Dividing by 2 (multiplicative decay)\n",
        "   - Or applying square root (slower decay)\n",
        "4. **Compare strategies**: Fixed LR, step decay, adaptive LR\n",
        "\n",
        "## Hypothesis\n",
        "\n",
        "Adaptive learning rates can improve training stability by automatically responding to optimization difficulties, potentially achieving better convergence than fixed or manually scheduled learning rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set: {X_train_scaled.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adaptive Learning Rate Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearRegressionAdaptiveLR:\n",
        "    \"\"\"Linear Regression with adaptive learning rate.\"\"\"\n",
        "    \n",
        "    def __init__(self, initial_lr=0.1, n_iterations=1000, batch_size=32, \n",
        "                 spike_threshold=1.1, decay_method='divide', random_state=42):\n",
        "        self.initial_lr = initial_lr\n",
        "        self.n_iterations = n_iterations\n",
        "        self.batch_size = batch_size\n",
        "        self.spike_threshold = spike_threshold  # Loss increase threshold\n",
        "        self.decay_method = decay_method  # 'divide' or 'sqrt'\n",
        "        self.random_state = random_state\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []\n",
        "        self.lr_history = []\n",
        "        \n",
        "    def _initialize_parameters(self, n_features):\n",
        "        np.random.seed(self.random_state)\n",
        "        self.weights = np.random.randn(n_features) * 0.01\n",
        "        self.bias = 0.0\n",
        "        \n",
        "    def _compute_loss(self, X, y):\n",
        "        predictions = X @ self.weights + self.bias\n",
        "        return np.mean((y - predictions) ** 2)\n",
        "    \n",
        "    def _compute_gradients(self, X_batch, y_batch):\n",
        "        n_samples = X_batch.shape[0]\n",
        "        predictions = X_batch @ self.weights + self.bias\n",
        "        error = predictions - y_batch\n",
        "        dw = (2 / n_samples) * (X_batch.T @ error)\n",
        "        db = (2 / n_samples) * np.sum(error)\n",
        "        return dw, db\n",
        "    \n",
        "    def _adjust_learning_rate(self, current_lr, previous_loss, current_loss):\n",
        "        \"\"\"Adjust learning rate based on loss spike detection.\"\"\"\n",
        "        if previous_loss is None:\n",
        "            return current_lr\n",
        "        \n",
        "        # Detect spike: loss increased by more than threshold\n",
        "        loss_ratio = current_loss / previous_loss if previous_loss > 0 else 1.0\n",
        "        \n",
        "        if loss_ratio > self.spike_threshold:\n",
        "            # Reduce learning rate\n",
        "            if self.decay_method == 'divide':\n",
        "                new_lr = current_lr / 2.0\n",
        "            elif self.decay_method == 'sqrt':\n",
        "                new_lr = current_lr * np.sqrt(0.5)\n",
        "            else:\n",
        "                new_lr = current_lr\n",
        "            \n",
        "            # Minimum learning rate threshold\n",
        "            new_lr = max(new_lr, 1e-6)\n",
        "            return new_lr\n",
        "        \n",
        "        return current_lr\n",
        "    \n",
        "    def fit(self, X, y, verbose=True):\n",
        "        \"\"\"Train with adaptive learning rate.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self._initialize_parameters(n_features)\n",
        "        \n",
        "        current_lr = self.initial_lr\n",
        "        previous_loss = None\n",
        "        \n",
        "        for iteration in range(self.n_iterations):\n",
        "            # Shuffle data\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "            \n",
        "            # Mini-batch gradient descent\n",
        "            epoch_losses = []\n",
        "            for i in range(0, n_samples, self.batch_size):\n",
        "                end_idx = min(i + self.batch_size, n_samples)\n",
        "                X_batch = X_shuffled[i:end_idx]\n",
        "                y_batch = y_shuffled[i:end_idx]\n",
        "                \n",
        "                # Compute gradients\n",
        "                dw, db = self._compute_gradients(X_batch, y_batch)\n",
        "                \n",
        "                # Update parameters\n",
        "                self.weights -= current_lr * dw\n",
        "                self.bias -= current_lr * db\n",
        "                \n",
        "                batch_loss = self._compute_loss(X_batch, y_batch)\n",
        "                epoch_losses.append(batch_loss)\n",
        "            \n",
        "            # Average loss for epoch\n",
        "            avg_loss = np.mean(epoch_losses)\n",
        "            \n",
        "            # Adjust learning rate based on loss spike\n",
        "            current_lr = self._adjust_learning_rate(current_lr, previous_loss, avg_loss)\n",
        "            previous_loss = avg_loss\n",
        "            \n",
        "            # Store history\n",
        "            self.loss_history.append(avg_loss)\n",
        "            self.lr_history.append(current_lr)\n",
        "            \n",
        "            if verbose and (iteration + 1) % 100 == 0:\n",
        "                print(f\"Iteration {iteration + 1}/{self.n_iterations}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return X @ self.weights + self.bias\n",
        "\n",
        "print(\"LinearRegressionAdaptiveLR defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline: Fixed Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearRegressionFixedLR:\n",
        "    \"\"\"Baseline: Fixed learning rate.\"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000, batch_size=32, random_state=42):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.batch_size = batch_size\n",
        "        self.random_state = random_state\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []\n",
        "        \n",
        "    def _initialize_parameters(self, n_features):\n",
        "        np.random.seed(self.random_state)\n",
        "        self.weights = np.random.randn(n_features) * 0.01\n",
        "        self.bias = 0.0\n",
        "        \n",
        "    def _compute_loss(self, X, y):\n",
        "        predictions = X @ self.weights + self.bias\n",
        "        return np.mean((y - predictions) ** 2)\n",
        "    \n",
        "    def _compute_gradients(self, X_batch, y_batch):\n",
        "        n_samples = X_batch.shape[0]\n",
        "        predictions = X_batch @ self.weights + self.bias\n",
        "        error = predictions - y_batch\n",
        "        dw = (2 / n_samples) * (X_batch.T @ error)\n",
        "        db = (2 / n_samples) * np.sum(error)\n",
        "        return dw, db\n",
        "    \n",
        "    def fit(self, X, y, verbose=True):\n",
        "        n_samples, n_features = X.shape\n",
        "        self._initialize_parameters(n_features)\n",
        "        \n",
        "        for iteration in range(self.n_iterations):\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "            \n",
        "            epoch_losses = []\n",
        "            for i in range(0, n_samples, self.batch_size):\n",
        "                end_idx = min(i + self.batch_size, n_samples)\n",
        "                X_batch = X_shuffled[i:end_idx]\n",
        "                y_batch = y_shuffled[i:end_idx]\n",
        "                \n",
        "                dw, db = self._compute_gradients(X_batch, y_batch)\n",
        "                self.weights -= self.learning_rate * dw\n",
        "                self.bias -= self.learning_rate * db\n",
        "                \n",
        "                batch_loss = self._compute_loss(X_batch, y_batch)\n",
        "                epoch_losses.append(batch_loss)\n",
        "            \n",
        "            avg_loss = np.mean(epoch_losses)\n",
        "            self.loss_history.append(avg_loss)\n",
        "            \n",
        "            if verbose and (iteration + 1) % 100 == 0:\n",
        "                print(f\"Iteration {iteration + 1}/{self.n_iterations}, Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return X @ self.weights + self.bias\n",
        "\n",
        "# Train fixed LR baseline\n",
        "print(\"Training fixed LR baseline...\")\n",
        "model_fixed = LinearRegressionFixedLR(learning_rate=0.01, n_iterations=1000, random_state=42)\n",
        "model_fixed.fit(X_train_scaled, y_train, verbose=True)\n",
        "\n",
        "y_pred_fixed = model_fixed.predict(X_test_scaled)\n",
        "test_rmse_fixed = np.sqrt(mean_squared_error(y_test, y_pred_fixed))\n",
        "print(f\"\\nFixed LR Test RMSE: {test_rmse_fixed:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step Decay Learning Rate (Comparison Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearRegressionStepDecay:\n",
        "    \"\"\"Step decay learning rate schedule.\"\"\"\n",
        "    \n",
        "    def __init__(self, initial_lr=0.1, decay_rate=0.5, decay_steps=250, \n",
        "                 n_iterations=1000, batch_size=32, random_state=42):\n",
        "        self.initial_lr = initial_lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.n_iterations = n_iterations\n",
        "        self.batch_size = batch_size\n",
        "        self.random_state = random_state\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []\n",
        "        self.lr_history = []\n",
        "        \n",
        "    def _initialize_parameters(self, n_features):\n",
        "        np.random.seed(self.random_state)\n",
        "        self.weights = np.random.randn(n_features) * 0.01\n",
        "        self.bias = 0.0\n",
        "        \n",
        "    def _compute_loss(self, X, y):\n",
        "        predictions = X @ self.weights + self.bias\n",
        "        return np.mean((y - predictions) ** 2)\n",
        "    \n",
        "    def _compute_gradients(self, X_batch, y_batch):\n",
        "        n_samples = X_batch.shape[0]\n",
        "        predictions = X_batch @ self.weights + self.bias\n",
        "        error = predictions - y_batch\n",
        "        dw = (2 / n_samples) * (X_batch.T @ error)\n",
        "        db = (2 / n_samples) * np.sum(error)\n",
        "        return dw, db\n",
        "    \n",
        "    def _get_learning_rate(self, iteration):\n",
        "        \"\"\"Step decay: reduce LR every decay_steps.\"\"\"\n",
        "        step = iteration // self.decay_steps\n",
        "        return self.initial_lr * (self.decay_rate ** step)\n",
        "    \n",
        "    def fit(self, X, y, verbose=True):\n",
        "        n_samples, n_features = X.shape\n",
        "        self._initialize_parameters(n_features)\n",
        "        \n",
        "        for iteration in range(self.n_iterations):\n",
        "            current_lr = self._get_learning_rate(iteration)\n",
        "            \n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "            \n",
        "            epoch_losses = []\n",
        "            for i in range(0, n_samples, self.batch_size):\n",
        "                end_idx = min(i + self.batch_size, n_samples)\n",
        "                X_batch = X_shuffled[i:end_idx]\n",
        "                y_batch = y_shuffled[i:end_idx]\n",
        "                \n",
        "                dw, db = self._compute_gradients(X_batch, y_batch)\n",
        "                self.weights -= current_lr * dw\n",
        "                self.bias -= current_lr * db\n",
        "                \n",
        "                batch_loss = self._compute_loss(X_batch, y_batch)\n",
        "                epoch_losses.append(batch_loss)\n",
        "            \n",
        "            avg_loss = np.mean(epoch_losses)\n",
        "            self.loss_history.append(avg_loss)\n",
        "            self.lr_history.append(current_lr)\n",
        "            \n",
        "            if verbose and (iteration + 1) % 100 == 0:\n",
        "                print(f\"Iteration {iteration + 1}/{self.n_iterations}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return X @ self.weights + self.bias\n",
        "\n",
        "# Train step decay\n",
        "print(\"Training step decay model...\")\n",
        "model_step = LinearRegressionStepDecay(initial_lr=0.01, decay_steps=250, n_iterations=1000, random_state=42)\n",
        "model_step.fit(X_train_scaled, y_train, verbose=True)\n",
        "\n",
        "y_pred_step = model_step.predict(X_test_scaled)\n",
        "test_rmse_step = np.sqrt(mean_squared_error(y_test, y_pred_step))\n",
        "print(f\"\\nStep Decay Test RMSE: {test_rmse_step:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adaptive Learning Rate Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train adaptive LR model\n",
        "print(\"Training adaptive LR model...\")\n",
        "model_adaptive = LinearRegressionAdaptiveLR(\n",
        "    initial_lr=0.1,\n",
        "    n_iterations=1000,\n",
        "    spike_threshold=1.1,\n",
        "    decay_method='divide',\n",
        "    random_state=42\n",
        ")\n",
        "model_adaptive.fit(X_train_scaled, y_train, verbose=True)\n",
        "\n",
        "y_pred_adaptive = model_adaptive.predict(X_test_scaled)\n",
        "test_rmse_adaptive = np.sqrt(mean_squared_error(y_test, y_pred_adaptive))\n",
        "print(f\"\\nAdaptive LR Test RMSE: {test_rmse_adaptive:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results using pickle\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "results_dir = os.path.join(os.path.dirname(os.getcwd()), 'results')\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "adaptive_results = {\n",
        "    'fixed_lr': {\n",
        "        'method': 'Fixed LR',\n",
        "        'test_rmse': test_rmse_fixed,\n",
        "        'loss_history': model_fixed.loss_history.copy(),\n",
        "        'learning_rate': 0.01\n",
        "    },\n",
        "    'step_decay': {\n",
        "        'method': 'Step Decay',\n",
        "        'test_rmse': test_rmse_step,\n",
        "        'loss_history': model_step.loss_history.copy(),\n",
        "        'lr_history': model_step.lr_history.copy()\n",
        "    },\n",
        "    'adaptive_lr': {\n",
        "        'method': 'Adaptive LR',\n",
        "        'test_rmse': test_rmse_adaptive,\n",
        "        'loss_history': model_adaptive.loss_history.copy(),\n",
        "        'lr_history': model_adaptive.lr_history.copy()\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(results_dir, '03_adaptive_lr_results.pkl'), 'wb') as f:\n",
        "    pickle.dump(adaptive_results, f)\n",
        "\n",
        "print(f\"\\nResults saved to: {os.path.join(results_dir, '03_adaptive_lr_results.pkl')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization: Learning Rate and Loss Dynamics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "# Loss comparison\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(model_fixed.loss_history, label='Fixed LR (0.01)', linewidth=2)\n",
        "plt.plot(model_step.loss_history, label='Step Decay', linewidth=2)\n",
        "plt.plot(model_adaptive.loss_history, label='Adaptive LR', linewidth=2)\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('MSE Loss', fontsize=12)\n",
        "plt.title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate comparison\n",
        "plt.subplot(2, 2, 2)\n",
        "fixed_lr_hist = [0.01] * len(model_fixed.loss_history)\n",
        "plt.plot(fixed_lr_hist, label='Fixed LR', linewidth=2, alpha=0.7)\n",
        "plt.plot(model_step.lr_history, label='Step Decay', linewidth=2)\n",
        "plt.plot(model_adaptive.lr_history, label='Adaptive LR', linewidth=2)\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('Learning Rate', fontsize=12)\n",
        "plt.title('Learning Rate Schedules', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Test performance comparison\n",
        "plt.subplot(2, 2, 3)\n",
        "methods = ['Fixed LR', 'Step Decay', 'Adaptive LR']\n",
        "rmses = [test_rmse_fixed, test_rmse_step, test_rmse_adaptive]\n",
        "plt.bar(methods, rmses, color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.7, edgecolor='black')\n",
        "plt.ylabel('Test RMSE', fontsize=12)\n",
        "plt.title('Test Performance Comparison', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Convergence speed (loss reduction over first 200 iterations)\n",
        "plt.subplot(2, 2, 4)\n",
        "window = 200\n",
        "fixed_reduction = (model_fixed.loss_history[0] - model_fixed.loss_history[window-1]) / model_fixed.loss_history[0]\n",
        "step_reduction = (model_step.loss_history[0] - model_step.loss_history[window-1]) / model_step.loss_history[0]\n",
        "adaptive_reduction = (model_adaptive.loss_history[0] - model_adaptive.loss_history[window-1]) / model_adaptive.loss_history[0]\n",
        "\n",
        "reductions = [fixed_reduction, step_reduction, adaptive_reduction]\n",
        "plt.bar(methods, reductions, color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.7, edgecolor='black')\n",
        "plt.ylabel('Loss Reduction (First 200 iters)', fontsize=12)\n",
        "plt.title('Convergence Speed Comparison', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(f\"Fixed LR Test RMSE: {test_rmse_fixed:.4f}\")\n",
        "print(f\"Step Decay Test RMSE: {test_rmse_step:.4f}\")\n",
        "print(f\"Adaptive LR Test RMSE: {test_rmse_adaptive:.4f}\")\n",
        "print(f\"\\nFinal Learning Rates:\")\n",
        "print(f\"Fixed LR: 0.01\")\n",
        "print(f\"Step Decay: {model_step.lr_history[-1]:.6f}\")\n",
        "print(f\"Adaptive LR: {model_adaptive.lr_history[-1]:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis and Conclusions\n",
        "\n",
        "### Key Observations:\n",
        "\n",
        "1. **Adaptive Learning Rate**: Automatically responds to loss spikes, potentially improving stability\n",
        "2. **Convergence Speed**: Adaptive LR may converge faster by starting high and reducing when needed\n",
        "3. **Stability**: Dynamic adjustment helps avoid oscillations and divergence\n",
        "\n",
        "### Research Insights:\n",
        "\n",
        "- Adaptive learning rates offer a middle ground between fixed and scheduled approaches\n",
        "- The spike detection mechanism provides automatic regularization\n",
        "- Further tuning of spike thresholds and decay methods could optimize performance"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
