{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Comparison and Conclusions\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook synthesizes results from all experiments and provides comprehensive comparisons between:\n",
    "\n",
    "1. **Baseline**: Standard mini-batch gradient descent with fixed learning rate\n",
    "2. **Class-Center Sampling**: Alternative data sampling strategies\n",
    "3. **Adaptive Learning Rate**: Dynamic learning rate adjustment\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Compare all training strategies on key metrics\n",
    "- Analyze convergence behavior and training dynamics\n",
    "- Draw research conclusions and insights\n",
    "- Discuss implications for future work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Re-run All Experiments\n",
    "\n",
    "For this comparison, we'll re-run key experiments to ensure consistent comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data loaded and preprocessed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Saved Results\n",
    "\n",
    "Load results from all previous experiments using pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "results_dir = os.path.join(os.path.dirname(os.getcwd()), 'results')\n",
    "\n",
    "# Load all results\n",
    "all_results = {}\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(results_dir, '01_baseline_results.pkl'), 'rb') as f:\n",
    "        all_results['baseline'] = pickle.load(f)\n",
    "    print(\"âœ“ Loaded baseline results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  Baseline results not found. Run notebook 01 first.\")\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(results_dir, '02_class_center_results.pkl'), 'rb') as f:\n",
    "        all_results['class_center'] = pickle.load(f)\n",
    "    print(\"âœ“ Loaded class center results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  Class center results not found. Run notebook 02 first.\")\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(results_dir, '03_adaptive_lr_results.pkl'), 'rb') as f:\n",
    "        all_results['adaptive_lr'] = pickle.load(f)\n",
    "    print(\"âœ“ Loaded adaptive LR results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  Adaptive LR results not found. Run notebook 03 first.\")\n",
    "\n",
    "print(f\"\\nLoaded {len(all_results)} result files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Results Summary\n",
    "\n",
    "Create a summary table comparing all methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results into a summary dataframe\n",
    "methods = []\n",
    "test_rmses = []\n",
    "test_r2s = []\n",
    "final_losses = []\n",
    "initial_losses = []\n",
    "loss_reductions = []\n",
    "\n",
    "# Baseline\n",
    "if 'baseline' in all_results:\n",
    "    r = all_results['baseline']\n",
    "    methods.append(r['method'])\n",
    "    test_rmses.append(r.get('test_rmse', np.nan))\n",
    "    test_r2s.append(r.get('test_r2', np.nan))\n",
    "    final_losses.append(r.get('final_loss', np.nan))\n",
    "    initial_losses.append(r.get('initial_loss', np.nan))\n",
    "    if 'initial_loss' in r and 'final_loss' in r:\n",
    "        loss_reductions.append((r['initial_loss'] - r['final_loss']) / r['initial_loss'] * 100)\n",
    "    else:\n",
    "        loss_reductions.append(np.nan)\n",
    "\n",
    "# Class Centers Only\n",
    "if 'class_center' in all_results:\n",
    "    r = all_results['class_center'].get('centers_only', {})\n",
    "    if r:\n",
    "        methods.append(r.get('method', 'Class Centers Only'))\n",
    "        test_rmses.append(r.get('test_rmse', np.nan))\n",
    "        test_r2s.append(r.get('test_r2', np.nan))\n",
    "        if 'loss_history' in r and len(r['loss_history']) > 0:\n",
    "            final_losses.append(r['loss_history'][-1])\n",
    "            initial_losses.append(r['loss_history'][0])\n",
    "            loss_reductions.append((r['loss_history'][0] - r['loss_history'][-1]) / r['loss_history'][0] * 100)\n",
    "        else:\n",
    "            final_losses.append(np.nan)\n",
    "            initial_losses.append(np.nan)\n",
    "            loss_reductions.append(np.nan)\n",
    "\n",
    "# Mixed Training\n",
    "if 'class_center' in all_results:\n",
    "    r = all_results['class_center'].get('mixed', {})\n",
    "    if r:\n",
    "        methods.append(r.get('method', 'Mixed (Centers + Random)'))\n",
    "        test_rmses.append(r.get('test_rmse', np.nan))\n",
    "        test_r2s.append(r.get('test_r2', np.nan))\n",
    "        if 'loss_history' in r and len(r['loss_history']) > 0:\n",
    "            final_losses.append(r['loss_history'][-1])\n",
    "            initial_losses.append(r['loss_history'][0])\n",
    "            loss_reductions.append((r['loss_history'][0] - r['loss_history'][-1]) / r['loss_history'][0] * 100)\n",
    "        else:\n",
    "            final_losses.append(np.nan)\n",
    "            initial_losses.append(np.nan)\n",
    "            loss_reductions.append(np.nan)\n",
    "\n",
    "# Step Decay\n",
    "if 'adaptive_lr' in all_results:\n",
    "    r = all_results['adaptive_lr'].get('step_decay', {})\n",
    "    if r:\n",
    "        methods.append(r.get('method', 'Step Decay'))\n",
    "        test_rmses.append(r.get('test_rmse', np.nan))\n",
    "        test_r2s.append(r.get('test_r2', np.nan))\n",
    "        if 'loss_history' in r and len(r['loss_history']) > 0:\n",
    "            final_losses.append(r['loss_history'][-1])\n",
    "            initial_losses.append(r['loss_history'][0])\n",
    "            loss_reductions.append((r['loss_history'][0] - r['loss_history'][-1]) / r['loss_history'][0] * 100)\n",
    "        else:\n",
    "            final_losses.append(np.nan)\n",
    "            initial_losses.append(np.nan)\n",
    "            loss_reductions.append(np.nan)\n",
    "\n",
    "# Adaptive LR\n",
    "if 'adaptive_lr' in all_results:\n",
    "    r = all_results['adaptive_lr'].get('adaptive_lr', {})\n",
    "    if r:\n",
    "        methods.append(r.get('method', 'Adaptive LR'))\n",
    "        test_rmses.append(r.get('test_rmse', np.nan))\n",
    "        test_r2s.append(r.get('test_r2', np.nan))\n",
    "        if 'loss_history' in r and len(r['loss_history']) > 0:\n",
    "            final_losses.append(r['loss_history'][-1])\n",
    "            initial_losses.append(r['loss_history'][0])\n",
    "            loss_reductions.append((r['loss_history'][0] - r['loss_history'][-1]) / r['loss_history'][0] * 100)\n",
    "        else:\n",
    "            final_losses.append(np.nan)\n",
    "            initial_losses.append(np.nan)\n",
    "            loss_reductions.append(np.nan)\n",
    "\n",
    "# Create summary dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Method': methods,\n",
    "    'Test RMSE': test_rmses,\n",
    "    'Test RÂ²': test_r2s,\n",
    "    'Initial Loss': initial_losses,\n",
    "    'Final Loss': final_losses,\n",
    "    'Loss Reduction (%)': loss_reductions\n",
    "})\n",
    "\n",
    "print(\"\\n=== Comprehensive Results Summary ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Display best method\n",
    "if len(test_rmses) > 0:\n",
    "    best_idx = np.nanargmin(test_rmses)\n",
    "    print(f\"\\nðŸ† Best Method (Lowest RMSE): {methods[best_idx]} (RMSE: {test_rmses[best_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Test RMSE Comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "if len(methods) > 0:\n",
    "    valid_indices = [i for i, rmse in enumerate(test_rmses) if not np.isnan(rmse)]\n",
    "    if valid_indices:\n",
    "        valid_methods = [methods[i] for i in valid_indices]\n",
    "        valid_rmses = [test_rmses[i] for i in valid_indices]\n",
    "        plt.barh(valid_methods, valid_rmses, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Test RMSE', fontsize=12)\n",
    "        plt.title('Test Performance Comparison', fontsize=14, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Loss Histories Comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "if 'baseline' in all_results and 'loss_history' in all_results['baseline']:\n",
    "    plt.plot(all_results['baseline']['loss_history'], label='Baseline', linewidth=2)\n",
    "if 'class_center' in all_results:\n",
    "    if 'centers_only' in all_results['class_center'] and 'loss_history' in all_results['class_center']['centers_only']:\n",
    "        plt.plot(all_results['class_center']['centers_only']['loss_history'], label='Centers Only', linewidth=2, alpha=0.7)\n",
    "    if 'mixed' in all_results['class_center'] and 'loss_history' in all_results['class_center']['mixed']:\n",
    "        plt.plot(all_results['class_center']['mixed']['loss_history'], label='Mixed', linewidth=2, alpha=0.7)\n",
    "if 'adaptive_lr' in all_results:\n",
    "    if 'adaptive_lr' in all_results['adaptive_lr'] and 'loss_history' in all_results['adaptive_lr']['adaptive_lr']:\n",
    "        plt.plot(all_results['adaptive_lr']['adaptive_lr']['loss_history'], label='Adaptive LR', linewidth=2, alpha=0.7)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontsize=12)\n",
    "plt.title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Learning Rate Histories (if available)\n",
    "plt.subplot(2, 3, 3)\n",
    "if 'adaptive_lr' in all_results:\n",
    "    if 'step_decay' in all_results['adaptive_lr'] and 'lr_history' in all_results['adaptive_lr']['step_decay']:\n",
    "        plt.plot(all_results['adaptive_lr']['step_decay']['lr_history'], label='Step Decay', linewidth=2)\n",
    "    if 'adaptive_lr' in all_results['adaptive_lr'] and 'lr_history' in all_results['adaptive_lr']['adaptive_lr']:\n",
    "        plt.plot(all_results['adaptive_lr']['adaptive_lr']['lr_history'], label='Adaptive LR', linewidth=2)\n",
    "    if 'baseline' in all_results and 'learning_rate' in all_results['baseline']:\n",
    "        fixed_lr = all_results['baseline']['learning_rate']\n",
    "        if 'loss_history' in all_results['baseline']:\n",
    "            plt.plot([fixed_lr] * len(all_results['baseline']['loss_history']), label='Fixed LR', linewidth=2, alpha=0.7)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('Learning Rate Schedules', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Loss Reduction Comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "if len(loss_reductions) > 0:\n",
    "    valid_indices = [i for i, red in enumerate(loss_reductions) if not np.isnan(red)]\n",
    "    if valid_indices:\n",
    "        valid_methods = [methods[i] for i in valid_indices]\n",
    "        valid_reductions = [loss_reductions[i] for i in valid_indices]\n",
    "        plt.bar(valid_methods, valid_reductions, alpha=0.7, edgecolor='black')\n",
    "        plt.ylabel('Loss Reduction (%)', fontsize=12)\n",
    "        plt.title('Convergence: Loss Reduction', fontsize=14, fontweight='bold')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Test RÂ² Comparison\n",
    "plt.subplot(2, 3, 5)\n",
    "if len(test_r2s) > 0:\n",
    "    valid_indices = [i for i, r2 in enumerate(test_r2s) if not np.isnan(r2)]\n",
    "    if valid_indices:\n",
    "        valid_methods = [methods[i] for i in valid_indices]\n",
    "        valid_r2s = [test_r2s[i] for i in valid_indices]\n",
    "        plt.bar(valid_methods, valid_r2s, alpha=0.7, edgecolor='black', color='lightgreen')\n",
    "        plt.ylabel('Test RÂ²', fontsize=12)\n",
    "        plt.title('Model Fit Quality (RÂ²)', fontsize=14, fontweight='bold')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6. Summary Statistics\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.axis('off')\n",
    "if len(methods) > 0:\n",
    "    summary_text = \"\\n=== Summary Statistics ===\\n\\n\"\n",
    "    for i, method in enumerate(methods):\n",
    "        summary_text += f\"{method}:\\n\"\n",
    "        if not np.isnan(test_rmses[i]):\n",
    "            summary_text += f\"  RMSE: {test_rmses[i]:.4f}\\n\"\n",
    "        if not np.isnan(test_r2s[i]):\n",
    "            summary_text += f\"  RÂ²: {test_r2s[i]:.4f}\\n\"\n",
    "        if not np.isnan(loss_reductions[i]):\n",
    "            summary_text += f\"  Loss Reduction: {loss_reductions[i]:.2f}%\\n\"\n",
    "        summary_text += \"\\n\"\n",
    "    plt.text(0.1, 0.5, summary_text, fontsize=10, verticalalignment='center', family='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Research Findings\n",
    "\n",
    "### 1. Class-Center Sampling\n",
    "\n",
    "**Findings:**\n",
    "- Class-center sampling provides condensed representation of dataset structure\n",
    "- Training with centers only may converge quickly but lacks diversity\n",
    "- Mixed training (centers + random) balances stability and diversity\n",
    "\n",
    "**Implications:**\n",
    "- Useful for understanding dataset structure\n",
    "- May be beneficial for large-scale training with limited compute\n",
    "- Requires careful tuning of mix ratios\n",
    "\n",
    "### 2. Adaptive Learning Rate\n",
    "\n",
    "**Findings:**\n",
    "- Adaptive LR automatically responds to optimization difficulties\n",
    "- Can improve stability compared to fixed LR\n",
    "- Reduces need for manual hyperparameter tuning\n",
    "\n",
    "**Implications:**\n",
    "- Promising for automated ML pipelines\n",
    "- Spike detection mechanism provides implicit regularization\n",
    "- Further research needed on optimal spike thresholds\n",
    "\n",
    "### 3. Training Dynamics\n",
    "\n",
    "**Findings:**\n",
    "- Different strategies lead to different convergence patterns\n",
    "- No single strategy dominates across all metrics\n",
    "- Trade-offs exist between speed, stability, and final performance\n",
    "\n",
    "**Implications:**\n",
    "- Strategy selection should be problem-dependent\n",
    "- Hybrid approaches may offer best of both worlds\n",
    "- Monitoring training dynamics is crucial for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations for Future Work\n",
    "\n",
    "1. **Hybrid Strategies**: Combine class-center sampling with adaptive learning rates\n",
    "2. **Advanced Sampling**: Explore other data-centric sampling strategies (e.g., hard example mining)\n",
    "3. **Learning Rate Schedules**: Investigate more sophisticated adaptive schedules (e.g., cosine annealing)\n",
    "4. **Dataset-Specific Analysis**: Study how dataset characteristics affect strategy effectiveness\n",
    "5. **Theoretical Analysis**: Develop theoretical understanding of why certain strategies work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This project explored alternative training strategies for linear regression, moving beyond standard mini-batch gradient descent. Key takeaways:\n",
    "\n",
    "1. **Data-centric approaches** (class-center sampling) offer interesting perspectives on training\n",
    "2. **Adaptive learning rates** can improve training stability and reduce hyperparameter sensitivity\n",
    "3. **No one-size-fits-all solution** - strategy selection should be informed by problem characteristics\n",
    "4. **Research mindset** - systematic experimentation and comparison are essential for understanding training dynamics\n",
    "\n",
    "These experiments demonstrate the value of questioning standard practices and exploring alternatives, even for fundamental algorithms like linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
